# pip install pycryptodome, pymupdf
from glob import glob
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.base import BaseCallbackHandler

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import (
    SystemMessage,
    HumanMessage,
    AIMessage
)
from langchain.callbacks import get_openai_callback

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

import json
import os

#QDRANT_PATH = "./local_qdrant"
COLLECTION_NAME = "my_collection_2"


def init_page():
    st.set_page_config(
        page_title="Ask My PDF(s)",
        page_icon="ğŸ¤—"
    )
    st.sidebar.title("Nav")
    st.session_state.costs = []


def select_model():
    model = st.sidebar.radio("Choose a model:", ( "GPT-3.5-16k", "GPT-4", "GPT-3.5"))
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
    elif model == "GPT-3.5-16k":
        st.session_state.model_name = "gpt-3.5-turbo-16k"
    else:
        st.session_state.model_name = "gpt-4"
    
    # 300: æœ¬æ–‡ä»¥å¤–ã®æŒ‡ç¤ºã®ãƒˆãƒ¼ã‚¯ãƒ³æ•° (ä»¥ä¸‹åŒã˜)
    st.session_state.max_token = OpenAI.modelname_to_contextsize(st.session_state.model_name) - 300
    return ChatOpenAI(temperature=0, model_name=st.session_state.model_name)

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = [
            SystemMessage(content="You are a helpful assistant.")
        ]
        st.session_state.costs = []

        #  memoryã®åˆæœŸåŒ–
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
        )

def get_pdf_text(uploaded_file):
    if uploaded_file:
        pdf_reader = PdfReader(uploaded_file)
        text = '\n\n'.join([page.extract_text() for page in pdf_reader.pages])
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name="text-embedding-ada-002",
            # é©åˆ‡ãª chunk size ã¯è³ªå•å¯¾è±¡ã®PDFã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ãŸã‚èª¿æ•´ãŒå¿…è¦
            # å¤§ããã—ã™ãã‚‹ã¨è³ªå•å›ç­”æ™‚ã«è‰²ã€…ãªç®‡æ‰€ã®æƒ…å ±ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ããªã„
            # é€†ã«å°ã•ã™ãã‚‹ã¨ä¸€ã¤ã®chunkã«ååˆ†ãªã‚µã‚¤ã‚ºã®æ–‡è„ˆãŒå…¥ã‚‰ãªã„
            chunk_size=500,
            chunk_overlap=0,
        )
        return text_splitter.split_text(text)
    else:
        return None


def load_qdrant():
    #client = QdrantClient(path=QDRANT_PATH)

    # ä»¥å‰ã“ã†æ›¸ã„ã¦ã„ãŸã¨ã“ã‚: client = QdrantClient(path=QDRANT_PATH)
    # url, api_key ã¯ Qdrant Cloud ã‹ã‚‰å–å¾—ã™ã‚‹
    client = QdrantClient(
        url=os.environ['QDRANT_CLOUD_ENDPOINT'],
        api_key=os.environ['QDRANT_CLOUD_API_KEY']
    )

    # ã™ã¹ã¦ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’å–å¾—
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆ
    if COLLECTION_NAME not in collection_names:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ–°ã—ãä½œæˆã—ã¾ã™
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )
        print('collection created')

    return Qdrant(
        client=client,
        collection_name=COLLECTION_NAME, 
        embeddings=OpenAIEmbeddings()
    )


def build_vector_store(pdf_text):
    qdrant = load_qdrant()
    qdrant.add_texts(pdf_text)

    # ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚‚ã§ãã‚‹ã€‚ã“ã®å ´åˆã¯æ¯å›ãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã‚‹
    # LangChain ã® Document Loader ã‚’åˆ©ç”¨ã—ãŸå ´åˆã¯ `from_documents` ã«ã™ã‚‹
    # Qdrant.from_texts(
    #     pdf_text,
    #     OpenAIEmbeddings(),
    #     path="./local_qdrant",
    #     collection_name="my_documents",
    # )


def build_qa_model(llm, memory):
    qdrant = load_qdrant()
    retriever = qdrant.as_retriever(
        # "mmr",  "similarity_score_threshold" ãªã©ã‚‚ã‚ã‚‹
        search_type="similarity",
        # æ–‡æ›¸ã‚’ä½•å€‹å–å¾—ã™ã‚‹ã‹ (default: 4)
        search_kwargs={"k":4}
    )

    return ConversationalRetrievalChain.from_llm(
       llm, retriever=retriever, memory=memory, verbose=True
    )


def page_pdf_upload_and_build_vector_db():
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ¬„ã‚’ä½œã‚Šã¾ã™ã€‚
    uploaded_file = st.sidebar.file_uploader(
            label='Upload your PDF hereğŸ˜‡',
            type='pdf'
        )
    if not uploaded_file:
            st.info("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
            st.stop()

    container = st.container()
    with container:
        pdf_text = get_pdf_text(uploaded_file)
        if pdf_text:
            with st.spinner("Loading PDF ..."):
                build_vector_store(pdf_text)


def ask(qa, query):
    with get_openai_callback() as cb:
        # query / result / source_documents
        answer = qa.run(query)

    return answer, cb.total_cost


def page_ask_my_pdf():

    llm = select_model()

    # ä¼šè©±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç®¡ç†ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚’è¨­å®šã™ã‚‹
    memory = st.session_state.memory 

    qa_chain = build_qa_model(llm, memory)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
        with st.spinner("ChatGPT is typing ..."):
            answer, cost = ask(qa_chain, user_input)
            st.session_state.costs.append(cost)

        st.session_state.messages.append(HumanMessage(content=user_input))
        st.session_state.messages.append(AIMessage(content=answer))

        messages = st.session_state.get('messages', [])
        for message in messages:
            if isinstance(message, AIMessage):
                with st.chat_message('assistant'):
                    st.markdown(message.content)
            elif isinstance(message, HumanMessage):
                with st.chat_message('user'):
                    st.markdown(message.content)
            else:  # isinstance(message, SystemMessage):
                st.write(f"System message: {message.content}")



# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›ã™ã‚‹é–¢æ•°
def message_to_dict(message):
    return {
        'type': type(message).__name__,
        'content': message.content,
    }


def main():
    init_page()
    init_messages()

    st.title("PDFã¨å¯¾è©±ã—ã‚ˆã†ï¼")

    page_pdf_upload_and_build_vector_db()

    page_ask_my_pdf()

    costs = st.session_state.get('costs', [])
    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${sum(costs):.5f}**")
    for cost in costs:
        st.sidebar.markdown(f"- ${cost:.5f}")


    messages_as_dicts = [message_to_dict(message) for message in st.session_state.messages]

    # ensure_ascii=Falseã‚’è¨­å®š
    messages_str = json.dumps(messages_as_dicts, ensure_ascii=False)

    st.download_button(
        label="Download messages",
        data=messages_str.encode(),  # bytesã«å¤‰æ›
        file_name='messages.json',
        mime='application/json',
    )

if __name__ == '__main__':
    main()
